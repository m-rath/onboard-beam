Python, Apache Beam, GCP Dataflow Runner -- Batch ETL Jobs


Recent improvements to my initial AB_NYC_2019.csv assignment:
- 49885 rows from Cloud Storage to Big Query with no errors
- schema includes Date column in BigQuery
- newline characters (unwanted) replaced with spaces
- success with and without a custom template
- top take-aways:
    - beam.dataframe module should handle csv files easily when apache_beam releases 2.36 (2.35 was released last night)
    - beam.io.ReadFromText has trouble with unwanted newline characters in csv files
    - DictReader (from the 'csv' python library) is a great solution, but pre-pipeline (in-memory solution, only for small datasets)
- moving forward:
    - my ReadFromText version misses >100 broken rows, so I could use it to practice implementing the deadletter pattern, writing bad records to a separate table
    - my DictReader version is a perfect opportunity to practice a Splittable DoFn (I have a pretty good understanding already; this matters to me)

New skills from tackling the Big Query dynamic pivot pipeline in python:
- learned the differences between beam.Map and beam.ParDo
    - beam.Map is a convenience class, for simple manipulations
    - beam.ParDo (with a DoFn) allows for finer control of 'process' method and exposes 5 more methods, e.g. for logging starts and stops of processes, workers, bundles
- implemented PTransforms, whose 'expand' method can call multiple ParDo's and return multiple PCollections
    - arguments can pass to 'expand' and 'process' methods two ways...
    - ...via instantiation (in __init__ method), or via side-inputs of ParDo (bypassing __init__ method)
    - if a pipeline begins with a PTransform -- i.e., initial source is read from within 'expand' method -- the pipeline itself is passed as an argument to the PTransform

And much more.